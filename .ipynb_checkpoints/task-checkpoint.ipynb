{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ade1c8-8b16-4ea5-84da-4e79eb4f323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# End-to-end implementation for the interview task\n",
    "# - Loads train features + labels (50k rows)\n",
    "# - Aligns/merges safely\n",
    "# - Cleans columns (drops 100% missing + constant cols)\n",
    "# - Builds preprocessing for numeric + categorical\n",
    "# - Trains model (tries LightGBM -> XGBoost -> HistGB fallback)\n",
    "# - Evaluates with PR-AUC, ROC-AUC, LogLoss\n",
    "# - Finds best threshold on validation (max F1 by default)\n",
    "# - Retrains on full data and saves artifacts\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (edit if needed)\n",
    "# -----------------------------\n",
    "FEATURES_PATH = \"train (6).csv\"\n",
    "LABELS_PATH   = \"train_churn_labels.csv\"\n",
    "\n",
    "OUT_DIR = \"model_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f61e18d-3aff-4be1-a5b3-8d9e717fd72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Loading data...\n",
      "2) Cleaning columns...\n",
      "3) Adding missingness features...\n",
      "4) Train/validation split (stratified)...\n",
      "   Positive rate (train): 0.0735 (imbalance expected)\n",
      "5) Model selection...\n",
      "   Using model: lightgbm\n",
      "   scale_pos_weight ~ 12.61\n",
      "6) Hyperparameter search (lightweight)...\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[LightGBM] [Info] Number of positive: 2938, number of negative: 37062\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23169\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 3751\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Best CV PR-AUC: 0.2000481371590344\n",
      "Best params: {'model__subsample': 0.9, 'model__reg_lambda': 0.0, 'model__num_leaves': 31, 'model__n_estimators': 1500, 'model__learning_rate': 0.01, 'model__colsample_bytree': 0.9}\n",
      "7) Validation evaluation...\n",
      "   PR-AUC:  0.181010\n",
      "   ROC-AUC: 0.705237\n",
      "   LogLoss: 0.480231\n",
      "   Best threshold (max F1): 0.6064\n",
      "\n",
      "Classification report (val):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9443    0.8954    0.9192      9266\n",
      "           1     0.2018    0.3338    0.2515       734\n",
      "\n",
      "    accuracy                         0.8542     10000\n",
      "   macro avg     0.5731    0.6146    0.5854     10000\n",
      "weighted avg     0.8898    0.8542    0.8702     10000\n",
      "\n",
      "Confusion matrix (val):\n",
      "[[8297  969]\n",
      " [ 489  245]]\n",
      "8) Retrain best model on full data and save artifacts...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 71675 features, but LGBMClassifier is expecting 63722 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 312\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved metadata:\u001b[39m\u001b[38;5;124m\"\u001b[39m, meta_path)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 312\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[4], line 283\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    280\u001b[0m sample_weight_full \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_full\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, spw_full, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     best_pipe\u001b[38;5;241m.\u001b[39mfit(X_full, y_full, model__sample_weight\u001b[38;5;241m=\u001b[39msample_weight_full)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     best_pipe\u001b[38;5;241m.\u001b[39mfit(X_full, y_full)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1558\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[1;32m-> 1560\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m   1561\u001b[0m     X,\n\u001b[0;32m   1562\u001b[0m     _y,\n\u001b[0;32m   1563\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1564\u001b[0m     init_score\u001b[38;5;241m=\u001b[39minit_score,\n\u001b[0;32m   1565\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[0;32m   1566\u001b[0m     eval_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[0;32m   1567\u001b[0m     eval_sample_weight\u001b[38;5;241m=\u001b[39meval_sample_weight,\n\u001b[0;32m   1568\u001b[0m     eval_class_weight\u001b[38;5;241m=\u001b[39meval_class_weight,\n\u001b[0;32m   1569\u001b[0m     eval_init_score\u001b[38;5;241m=\u001b[39meval_init_score,\n\u001b[0;32m   1570\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39meval_metric,\n\u001b[0;32m   1571\u001b[0m     feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[0;32m   1572\u001b[0m     categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_feature,\n\u001b[0;32m   1573\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1574\u001b[0m     init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[0;32m   1575\u001b[0m )\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:949\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    946\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [metric \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (pd_DataFrame, dt_DataTable)):\n\u001b[1;32m--> 949\u001b[0m     _X, _y \u001b[38;5;241m=\u001b[39m _LGBMValidateData(\n\u001b[0;32m    950\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    951\u001b[0m         X,\n\u001b[0;32m    952\u001b[0m         y,\n\u001b[0;32m    953\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    954\u001b[0m         \u001b[38;5;66;03m# allow any input type (this validation is done further down, in lgb.Dataset())\u001b[39;00m\n\u001b[0;32m    955\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    956\u001b[0m         \u001b[38;5;66;03m# do not raise an error if Inf of NaN values are found (LightGBM handles these internally)\u001b[39;00m\n\u001b[0;32m    957\u001b[0m         ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;66;03m# raise an error on 0-row and 1-row inputs\u001b[39;00m\n\u001b[0;32m    959\u001b[0m         ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m _LGBMCheckSampleWeight(sample_weight, _X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\compat.py:91\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, accept_sparse, ensure_all_finite, ensure_min_samples, **ignored_kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# raise the same error that scikit-learn's `validate_data()` does on scikit-learn>=1.6\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _estimator\u001b[38;5;241m.\u001b[39m__sklearn_is_fitted__() \u001b[38;5;129;01mand\u001b[39;00m _estimator\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m!=\u001b[39m n_features_in_:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_estimator\u001b[38;5;241m.\u001b[39m_n_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_val_y:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: X has 71675 features, but LGBMClassifier is expecting 63722 features as input."
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Helper classes\n",
    "# -----------------------------\n",
    "class ToDense(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Convert sparse matrix to dense (required for HistGradientBoostingClassifier).\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else np.asarray(X)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def load_and_align(features_path: str, labels_path: str):\n",
    "    X = pd.read_csv(features_path)\n",
    "    y_df = pd.read_csv(labels_path)\n",
    "\n",
    "    possible_label_cols = [\"label\", \"Label\", \"target\", \"Target\", \"y\", \"Y\"]\n",
    "    label_col = None\n",
    "    for c in possible_label_cols:\n",
    "        if c in y_df.columns:\n",
    "            label_col = c\n",
    "            break\n",
    "    if label_col is None:\n",
    "        label_col = y_df.columns[0]\n",
    "\n",
    "    y_raw = y_df[label_col].copy()\n",
    "\n",
    "    if len(X) != len(y_raw):\n",
    "        raise ValueError(f\"Row mismatch: X={len(X)} but y={len(y_raw)}. Need ID-based join.\")\n",
    "\n",
    "    y = y_raw.replace({-1: 0, 1: 1}).astype(int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def drop_bad_columns(df: pd.DataFrame):\n",
    "    df2 = df.copy()\n",
    "    all_missing = [c for c in df2.columns if df2[c].isna().all()]\n",
    "    df2.drop(columns=all_missing, inplace=True)\n",
    "\n",
    "    constant = [c for c in df2.columns if df2[c].nunique(dropna=True) <= 1]\n",
    "    df2.drop(columns=constant, inplace=True)\n",
    "\n",
    "    dropped = {\"all_missing\": all_missing, \"constant\": constant}\n",
    "    return df2, dropped\n",
    "\n",
    "\n",
    "def add_missingness_features(df: pd.DataFrame):\n",
    "    df2 = df.copy()\n",
    "    miss_count = df2.isna().sum(axis=1)\n",
    "    miss_ratio = miss_count / max(df2.shape[1], 1)\n",
    "    df2[\"__missing_count__\"] = miss_count\n",
    "    df2[\"__missing_ratio__\"] = miss_ratio\n",
    "    return df2\n",
    "\n",
    "\n",
    "def get_feature_types(df: pd.DataFrame):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "def choose_model():\n",
    "    # --- LightGBM ---\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "        model = LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            n_estimators=2000,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=63,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        return \"lightgbm\", model\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- XGBoost ---\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        return \"xgboost\", model\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- Fallback: HistGradientBoosting ---\n",
    "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.06,\n",
    "        max_depth=6,\n",
    "        max_iter=600,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    return \"hist_gb\", model\n",
    "\n",
    "\n",
    "def build_pipeline(X: pd.DataFrame, base_model, model_name: str):\n",
    "    \"\"\"\n",
    "    Key fix:\n",
    "    - HistGradientBoostingClassifier requires dense input.\n",
    "    - So we add ToDense() right after preprocessing only for hist_gb.\n",
    "    \"\"\"\n",
    "    num_cols, cat_cols = get_feature_types(X)\n",
    "\n",
    "    numeric_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    ])\n",
    "\n",
    "    categorical_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipe, num_cols),\n",
    "            (\"cat\", categorical_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    steps = [(\"preprocess\", preprocessor)]\n",
    "\n",
    "    if model_name == \"hist_gb\":\n",
    "        steps.append((\"todense\", ToDense()))  # ✅ critical fix\n",
    "\n",
    "    steps.append((\"model\", base_model))\n",
    "\n",
    "    return Pipeline(steps=steps)\n",
    "\n",
    "\n",
    "def find_best_threshold(y_true, y_proba, method=\"max_f1\"):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    if len(thresholds) == 0:\n",
    "        return 0.5\n",
    "\n",
    "    if method == \"max_f1\":\n",
    "        f1s = (2 * precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-12)\n",
    "        best_idx = int(np.argmax(f1s))\n",
    "        return float(thresholds[best_idx])\n",
    "\n",
    "    return 0.5\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main training flow\n",
    "# -----------------------------\n",
    "def main():\n",
    "    print(\"1) Loading data...\")\n",
    "    X_raw, y = load_and_align(FEATURES_PATH, LABELS_PATH)\n",
    "\n",
    "    print(\"2) Cleaning columns...\")\n",
    "    X_clean, dropped = drop_bad_columns(X_raw)\n",
    "\n",
    "    print(\"3) Adding missingness features...\")\n",
    "    X_feat = add_missingness_features(X_clean)\n",
    "\n",
    "    print(\"4) Train/validation split (stratified)...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_feat, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    pos_rate = y_train.mean()\n",
    "    print(f\"   Positive rate (train): {pos_rate:.4f} (imbalance expected)\")\n",
    "\n",
    "    print(\"5) Model selection...\")\n",
    "    model_name, base_model = choose_model()\n",
    "    print(f\"   Using model: {model_name}\")\n",
    "\n",
    "    neg = int((y_train == 0).sum())\n",
    "    pos = int((y_train == 1).sum())\n",
    "    if pos == 0:\n",
    "        raise ValueError(\"No positive samples in training split.\")\n",
    "    scale_pos_weight = neg / pos\n",
    "    print(f\"   scale_pos_weight ~ {scale_pos_weight:.2f}\")\n",
    "\n",
    "    pipe = build_pipeline(X_train, base_model, model_name)\n",
    "\n",
    "    print(\"6) Hyperparameter search (lightweight)...\")\n",
    "    if model_name == \"lightgbm\":\n",
    "        param_dist = {\n",
    "            \"model__num_leaves\": [31, 63, 127],\n",
    "            \"model__learning_rate\": [0.01, 0.03, 0.06],\n",
    "            \"model__n_estimators\": [800, 1500, 2500],\n",
    "            \"model__subsample\": [0.7, 0.8, 0.9],\n",
    "            \"model__colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "            \"model__reg_lambda\": [0.0, 1.0, 5.0],\n",
    "        }\n",
    "    elif model_name == \"xgboost\":\n",
    "        param_dist = {\n",
    "            \"model__max_depth\": [4, 6, 8],\n",
    "            \"model__learning_rate\": [0.01, 0.03, 0.06],\n",
    "            \"model__n_estimators\": [800, 1500, 2500],\n",
    "            \"model__subsample\": [0.7, 0.8, 0.9],\n",
    "            \"model__colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "            \"model__reg_lambda\": [0.0, 1.0, 5.0],\n",
    "        }\n",
    "    else:\n",
    "        param_dist = {\n",
    "            \"model__learning_rate\": [0.03, 0.06, 0.1],\n",
    "            \"model__max_depth\": [4, 6, 8],\n",
    "            \"model__max_iter\": [300, 600, 900],\n",
    "        }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=min(15, sum(len(v) for v in param_dist.values())),\n",
    "        scoring=\"average_precision\",\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        error_score=\"raise\"   # ✅ so you see the REAL root error immediately\n",
    "    )\n",
    "\n",
    "    # sample weights (try to pass them; if estimator doesn't support, fall back)\n",
    "    sample_weight = np.where(y_train.values == 1, scale_pos_weight, 1.0)\n",
    "\n",
    "    try:\n",
    "        search.fit(X_train, y_train, model__sample_weight=sample_weight)\n",
    "    except TypeError:\n",
    "        search.fit(X_train, y_train)\n",
    "\n",
    "    best_pipe = search.best_estimator_\n",
    "    print(\"Best CV PR-AUC:\", search.best_score_)\n",
    "    print(\"Best params:\", search.best_params_)\n",
    "\n",
    "    print(\"7) Validation evaluation...\")\n",
    "    if hasattr(best_pipe, \"predict_proba\"):\n",
    "        y_val_proba = best_pipe.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        # fallback; most should have predict_proba\n",
    "        y_val_proba = best_pipe.predict(X_val)\n",
    "\n",
    "    pr_auc = average_precision_score(y_val, y_val_proba)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    ll = log_loss(y_val, y_val_proba)\n",
    "\n",
    "    print(f\"   PR-AUC:  {pr_auc:.6f}\")\n",
    "    print(f\"   ROC-AUC: {roc_auc:.6f}\")\n",
    "    print(f\"   LogLoss: {ll:.6f}\")\n",
    "\n",
    "    thr = find_best_threshold(y_val, y_val_proba, method=\"max_f1\")\n",
    "    print(f\"   Best threshold (max F1): {thr:.4f}\")\n",
    "\n",
    "    y_val_pred = (y_val_proba >= thr).astype(int)\n",
    "\n",
    "    print(\"\\nClassification report (val):\")\n",
    "    print(classification_report(y_val, y_val_pred, digits=4))\n",
    "    print(\"Confusion matrix (val):\")\n",
    "    print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "    print(\"8) Retrain best model on full data and save artifacts...\")\n",
    "    X_full = X_feat\n",
    "    y_full = y\n",
    "\n",
    "    neg_full = int((y_full == 0).sum())\n",
    "    pos_full = int((y_full == 1).sum())\n",
    "    spw_full = neg_full / max(pos_full, 1)\n",
    "    sample_weight_full = np.where(y_full.values == 1, spw_full, 1.0)\n",
    "\n",
    "    try:\n",
    "        best_pipe.fit(X_full, y_full, model__sample_weight=sample_weight_full)\n",
    "    except TypeError:\n",
    "        best_pipe.fit(X_full, y_full)\n",
    "\n",
    "    model_path = os.path.join(OUT_DIR, f\"final_model_{model_name}.joblib\")\n",
    "    joblib.dump(best_pipe, model_path)\n",
    "\n",
    "    meta = {\n",
    "        \"model_name\": model_name,\n",
    "        \"dropped_columns\": dropped,\n",
    "        \"best_params\": search.best_params_,\n",
    "        \"val_metrics\": {\n",
    "            \"pr_auc\": float(pr_auc),\n",
    "            \"roc_auc\": float(roc_auc),\n",
    "            \"log_loss\": float(ll),\n",
    "            \"threshold_max_f1\": float(thr)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    meta_path = os.path.join(OUT_DIR, \"training_metadata.json\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(\"\\nDONE ✅\")\n",
    "    print(\"Saved model:\", model_path)\n",
    "    print(\"Saved metadata:\", meta_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
